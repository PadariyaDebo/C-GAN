{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PATEGAN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO30dX1C4Rkb1KChl5fZiHQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PadariyaDebo/CGAN-DCGAN-WGAN/blob/main/PATEGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oG7vB66Jf597",
        "outputId": "098d6158-2080-454a-f914-b58ae64de97c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mlforhealthlabpub'...\n",
            "remote: Enumerating objects: 1192, done.\u001b[K\n",
            "remote: Counting objects: 100% (128/128), done.\u001b[K\n",
            "remote: Compressing objects: 100% (84/84), done.\u001b[K\n",
            "remote: Total 1192 (delta 67), reused 48 (delta 44), pack-reused 1064\u001b[K\n",
            "Receiving objects: 100% (1192/1192), 17.31 MiB | 8.64 MiB/s, done.\n",
            "Resolving deltas: 100% (268/268), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/PadariyaDebo/mlforhealthlabpub.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy\n",
        "!pip install argparse\n",
        "!pip install pandas\n",
        "!pip install tensorflow==1.15.0\n",
        "!pip install scikit-learn\n",
        "!pip install xgboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2gF1zBUVg2ko",
        "outputId": "6de79949-294e-401d-8097-745da985b6b4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting argparse\n",
            "  Downloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
            "Installing collected packages: argparse\n",
            "Successfully installed argparse-1.4.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "argparse"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==1.15.0\n",
            "  Downloading tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3 MB 27 kB/s \n",
            "\u001b[?25hCollecting tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 56.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.2.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.37.1)\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 6.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.15.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.8.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (3.3.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.1.2)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (3.17.3)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 65.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.14.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.47.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.2.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.21.6)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.0) (3.1.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (57.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (4.1.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==1.15.0) (1.5.2)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=78139d3353726abb418ccdf88d6eb934651193e8fb90608ea9551958d0f98f67\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "Successfully built gast\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, gast, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.2+zzzcolab20220719082949\n",
            "    Uninstalling tensorflow-2.8.2+zzzcolab20220719082949:\n",
            "      Successfully uninstalled tensorflow-2.8.2+zzzcolab20220719082949\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.16.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n",
            "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.15.0 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.21.6)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.7/dist-packages (0.90)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.7.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/mlforhealthlabpub/alg/pategan\n",
        "!pip install -v -e ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMuVJrGThojE",
        "outputId": "919e25fd-630f-4a2e-f76c-95ea03d19a4a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/mlforhealthlabpub/alg/pategan\n",
            "Using pip 21.1.3 from /usr/local/lib/python3.7/dist-packages/pip (python 3.7)\n",
            "Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.7/dist-packages\n",
            "sysconfig: /usr/lib/python3.7/site-packages\n",
            "Value for scheme.purelib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.7/dist-packages\n",
            "sysconfig: /usr/lib/python3.7/site-packages\n",
            "Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/include/python3.7/UNKNOWN\n",
            "sysconfig: /usr/include/python3.7m/UNKNOWN\n",
            "Value for scheme.scripts does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/bin\n",
            "sysconfig: /usr/bin\n",
            "Value for scheme.data does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local\n",
            "sysconfig: /usr\n",
            "Additional context:\n",
            "user = False\n",
            "home = None\n",
            "root = None\n",
            "prefix = None\n",
            "Non-user install because site-packages writeable\n",
            "Created temporary directory: /tmp/pip-ephem-wheel-cache-gke8qbnm\n",
            "Created temporary directory: /tmp/pip-req-tracker-aqrem532\n",
            "Initialized build tracking at /tmp/pip-req-tracker-aqrem532\n",
            "Created build tracker: /tmp/pip-req-tracker-aqrem532\n",
            "Entered build tracker: /tmp/pip-req-tracker-aqrem532\n",
            "Created temporary directory: /tmp/pip-install-ua63j4n3\n",
            "\u001b[31mERROR: File \"setup.py\" or \"setup.cfg\" not found. Directory cannot be installed in editable mode: /content/mlforhealthlabpub/alg/pategan\u001b[0m\n",
            "Exception information:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/base_command.py\", line 180, in _main\n",
            "    status = self.run(options, args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/req_command.py\", line 199, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/commands/install.py\", line 289, in run\n",
            "    reqs = self.get_requirements(args, options, finder, session)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/req_command.py\", line 379, in get_requirements\n",
            "    use_pep517=options.use_pep517,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/req/constructors.py\", line 210, in install_req_from_editable\n",
            "    parts = parse_req_from_editable(editable_req)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/req/constructors.py\", line 181, in parse_req_from_editable\n",
            "    name, url, extras_override = parse_editable(editable_req)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/req/constructors.py\", line 94, in parse_editable\n",
            "    raise InstallationError(msg)\n",
            "pip._internal.exceptions.InstallationError: File \"setup.py\" or \"setup.cfg\" not found. Directory cannot be installed in editable mode: /content/mlforhealthlabpub/alg/pategan\n",
            "Removed build tracker: '/tmp/pip-req-tracker-aqrem532'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def data_generator(no, dim, noise_rate):\n",
        "  \"\"\"Generate train/test dataset for PATE-GAN evaluation\n",
        "  \n",
        "  Args:\n",
        "    no: The number of train/test samples\n",
        "    dim: The number of dimensions in train/test features\n",
        "    noise_rate: The amount of noise for generating labels\n",
        "    \n",
        "  Returns:\n",
        "    train_data: Training data (feature + label)\n",
        "    test_data: Testing data (feature + label)\n",
        "  \"\"\"\n",
        "  \n",
        "  # Define symmetric covariance matrix for generating features\n",
        "  cov_matrix = np.random.uniform(0, 1, [dim, dim])\n",
        "  cov_matrix = 0.5 * (cov_matrix + np.transpose(cov_matrix))\n",
        "  \n",
        "  # Generate train/test features\n",
        "  x_train = np.random.multivariate_normal(np.zeros([dim,]), cov_matrix, [no,])\n",
        "  x_test = np.random.multivariate_normal(np.zeros([dim,]), cov_matrix, [no,])\n",
        "  \n",
        "  # Define feature label relationship\n",
        "  W = np.random.uniform(0, 1, [dim,])\n",
        "  b = np.random.uniform(0, 1, 1)\n",
        "  \n",
        "  # Generate train/test labels\n",
        "  y_train = np.matmul(x_train, W) + b + np.random.normal(0, noise_rate, no)\n",
        "  y_train = np.reshape(1*(y_train), [-1, 1])\n",
        "  \n",
        "  y_test = np.matmul(x_test, W) + b + np.random.normal(0, noise_rate, no)\n",
        "  y_test = np.reshape(1*(y_test), [-1, 1])\n",
        "    \n",
        "  train_data = np.concatenate((x_train, y_train), axis=1)\n",
        "  test_data = np.concatenate((x_test, y_test), axis=1)\n",
        "  \n",
        "  # Normalization\n",
        "  for i in range(dim+1):\n",
        "    train_data[:, i] = train_data[:, i] - np.min(train_data[:, i])\n",
        "    train_data[:, i] = train_data[:, i] / (np.max(train_data[:, i]) + 1e-8)\n",
        "    \n",
        "  # Normalization\n",
        "  for i in range(dim+1):\n",
        "    test_data[:, i] = test_data[:, i] - np.min(test_data[:, i])\n",
        "    test_data[:, i] = test_data[:, i] / (np.max(test_data[:, i]) + 1e-8)\n",
        "  \n",
        "  return train_data, test_data"
      ],
      "metadata": {
        "id": "244HEO8hi_yN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "\n",
        "# Predictive models\n",
        "from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier\n",
        "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
        "from sklearn import svm\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "\n",
        "def supervised_model_training(x_train, y_train, x_test, \n",
        "                              y_test, model_name):\n",
        "  \"\"\"Train supervised learning models and report the results.\n",
        "  \n",
        "  Args:\n",
        "    - x_train, y_train: training dataset\n",
        "    - x_test, y_test: testing dataset\n",
        "    - model_name: supervised model name such as logisticregression\n",
        "    \n",
        "  Returns:\n",
        "    - auc: Area Under ROC Curve\n",
        "    - apr: Average Precision and Recall\n",
        "  \"\"\"\n",
        "     \n",
        "  if model_name == 'logisticregression':\n",
        "    model  = LogisticRegression()\n",
        "  elif model_name == 'randomforest':      \n",
        "    model = RandomForestClassifier()\n",
        "  elif model_name == 'gaussiannb':  \n",
        "    model = GaussianNB()\n",
        "  elif model_name == 'bernoullinb':  \n",
        "    model        = BernoulliNB()\n",
        "  elif model_name == 'multinb':  \n",
        "    model        = MultinomialNB()\n",
        "  elif model_name == 'svmlin':         \n",
        "    model        = svm.LinearSVC() \n",
        "  elif model_name == 'gbm':         \n",
        "    model         = GradientBoostingClassifier()   \n",
        "  elif model_name == 'Extra Trees':\n",
        "    model =  ExtraTreesClassifier(n_estimators=20)\n",
        "  elif model_name == 'LDA':\n",
        "    model =  LinearDiscriminantAnalysis() \n",
        "  elif model_name == 'Passive Aggressive':\n",
        "    model =   PassiveAggressiveClassifier()\n",
        "  elif model_name == 'AdaBoost':\n",
        "    model =   AdaBoostClassifier()\n",
        "  elif model_name == 'Bagging':\n",
        "    model =   BaggingClassifier()\n",
        "  elif model_name == 'xgb':\n",
        "    model =   XGBRegressor()                                \n",
        "  \n",
        "  if(model_name=='svmlin' or model_name=='Passive Aggressive'): \n",
        "    model.fit(x_train, y_train)\n",
        "    predict = model.decision_function(x_test)\n",
        "  elif (model_name =='xgb'):\n",
        "    model.fit(np.asarray(x_train), y_train)\n",
        "    predict = model.predict(np.asarray(x_test))\n",
        "  else:\n",
        "    model.fit(x_train, y_train)\n",
        "    predict = model.predict_proba(x_test)[:,1]\n",
        "        \n",
        "  # AUC / AUPRC Computation\n",
        "  auc = metrics.roc_auc_score(y_test, predict)\n",
        "  apr = metrics.average_precision_score(y_test, predict)\n",
        "  \n",
        "  return auc, apr    "
      ],
      "metadata": {
        "id": "CM4WmIOjjIfx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "def pate_lamda (x, teacher_models, lamda):\n",
        "  '''Returns PATE_lambda(x).\n",
        "  \n",
        "  Args:\n",
        "    - x: feature vector\n",
        "    - teacher_models: a list of teacher models\n",
        "    - lamda: parameter\n",
        "    \n",
        "  Returns:\n",
        "    - n0, n1: the number of label 0 and 1, respectively\n",
        "    - out: label after adding laplace noise.\n",
        "  '''\n",
        "      \n",
        "  y_hat = list()\n",
        "        \n",
        "  for teacher in teacher_models:            \n",
        "    temp_y = teacher.predict(np.reshape(x, [1,-1]))\n",
        "    y_hat = y_hat + [temp_y]\n",
        "  \n",
        "  y_hat = np.asarray(y_hat)\n",
        "  n0 = sum(y_hat == 0)\n",
        "  n1 = sum(y_hat == 1)\n",
        "  \n",
        "  lap_noise = np.random.laplace(loc=0.0, scale=lamda)\n",
        "  \n",
        "  out = (n1+lap_noise) / float(n0+n1)\n",
        "  out = int(out>0.5)\n",
        "        \n",
        "  return n0, n1, out \n",
        "\n",
        "\n",
        "def pategan(x_train, parameters):\n",
        "  '''Basic PATE-GAN framework.\n",
        "  \n",
        "  Args:\n",
        "    - x_train: training data\n",
        "    - parameters: PATE-GAN parameters\n",
        "      - n_s: the number of student training iterations\n",
        "      - batch_size: the number of batch size for training student and generator\n",
        "      - k: the number of teachers\n",
        "      - epsilon, delta: Differential privacy parameters\n",
        "      - lamda: noise size\n",
        "      \n",
        "  Returns:\n",
        "    - x_train_hat: generated training data by differentially private generator\n",
        "  '''\n",
        "  \n",
        "  # Reset the graph\n",
        "  tf.reset_default_graph()\n",
        "    \n",
        "  # PATE-GAN parameters\n",
        "  # number of student training iterations\n",
        "  n_s = parameters['n_s']\n",
        "  # number of batch size for student and generator training\n",
        "  batch_size = parameters['batch_size']\n",
        "  # number of teachers\n",
        "  k = parameters['k']\n",
        "  # epsilon\n",
        "  epsilon = parameters['epsilon']\n",
        "  # delta\n",
        "  delta = parameters['delta']\n",
        "  # lamda\n",
        "  lamda = parameters['lamda']\n",
        "  \n",
        "  # Other parameters\n",
        "  # alpha initialize\n",
        "  L = 20\n",
        "  alpha = np.zeros([L])\n",
        "  # initialize epsilon_hat\n",
        "  epsilon_hat = 0\n",
        "    \n",
        "  # Network parameters\n",
        "  no, dim = x_train.shape\n",
        "  # Random sample dimensions\n",
        "  z_dim = int(dim)\n",
        "  # Student hidden dimension\n",
        "  student_h_dim = int(dim)\n",
        "  # Generator hidden dimension\n",
        "  generator_h_dim = int(4*dim)  \n",
        "  \n",
        "  ## Partitioning the data into k subsets\n",
        "  x_partition = list()\n",
        "  partition_data_no = int(no/k)\n",
        "    \n",
        "  idx = np.random.permutation(no)\n",
        "    \n",
        "  for i in range(k):\n",
        "    temp_idx = idx[int(i*partition_data_no):int((i+1)*partition_data_no)]\n",
        "    temp_x = x_train[temp_idx, :]      \n",
        "    x_partition = x_partition + [temp_x]    \n",
        "  \n",
        "  ## Necessary Functions for buidling NN models\n",
        "  # Xavier Initialization Definition\n",
        "  def xavier_init(size):\n",
        "    in_dim = size[0]\n",
        "    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
        "    return tf.random_normal(shape = size, stddev = xavier_stddev)    \n",
        "        \n",
        "  # Sample from uniform distribution\n",
        "  def sample_Z(m, n):\n",
        "    return np.random.uniform(-1., 1., size = [m, n])\n",
        "     \n",
        "  ## Placeholder\n",
        "  # PATE labels\n",
        "  Y = tf.placeholder(tf.float32, shape = [None, 1])  \n",
        "  # Random Variable    \n",
        "  Z = tf.placeholder(tf.float32, shape = [None, z_dim])\n",
        "   \n",
        "  ## NN variables   \n",
        "  # Student\n",
        "  S_W1 = tf.Variable(xavier_init([dim, student_h_dim]))\n",
        "  S_b1 = tf.Variable(tf.zeros(shape=[student_h_dim]))\n",
        "    \n",
        "  S_W2 = tf.Variable(xavier_init([student_h_dim,1]))\n",
        "  S_b2 = tf.Variable(tf.zeros(shape=[1]))\n",
        "\n",
        "  theta_S = [S_W1, S_W2, S_b1, S_b2]\n",
        "    \n",
        "  # Generator\n",
        "\n",
        "  G_W1 = tf.Variable(xavier_init([z_dim, generator_h_dim]))\n",
        "  G_b1 = tf.Variable(tf.zeros(shape=[generator_h_dim]))\n",
        "\n",
        "  G_W2 = tf.Variable(xavier_init([generator_h_dim,generator_h_dim]))\n",
        "  G_b2 = tf.Variable(tf.zeros(shape=[generator_h_dim]))\n",
        "\n",
        "  G_W3 = tf.Variable(xavier_init([generator_h_dim,dim]))\n",
        "  G_b3 = tf.Variable(tf.zeros(shape=[dim]))\n",
        "    \n",
        "  theta_G = [G_W1, G_W2, G_W3, G_b1, G_b2, G_b3]\n",
        "\n",
        "  ## Models\n",
        "  def generator(z):\n",
        "    G_h1 = tf.nn.tanh(tf.matmul(z, G_W1) + G_b1)\n",
        "    G_h2 = tf.nn.tanh(tf.matmul(G_h1, G_W2) + G_b2)\n",
        "    G_out = tf.nn.sigmoid(tf.matmul(G_h2, G_W3) + G_b3)\n",
        "        \n",
        "    return G_out\n",
        "    \n",
        "  def student(x):\n",
        "    S_h1 = tf.nn.relu(tf.matmul(x, S_W1) + S_b1)\n",
        "    S_out = tf.matmul(S_h1, S_W2) + S_b2\n",
        "        \n",
        "    return S_out\n",
        "      \n",
        "  ## Loss  \n",
        "  G_sample = generator(Z)\n",
        "  S_fake = student(G_sample)\n",
        "  \n",
        "  S_loss = tf.reduce_mean(Y * S_fake) - tf.reduce_mean((1-Y) * S_fake)\n",
        "  G_loss = -tf.reduce_mean(S_fake)\n",
        "  \n",
        "  # Optimizer\n",
        "  S_solver = (tf.train.RMSPropOptimizer(learning_rate=1e-4)\n",
        "              .minimize(-S_loss, var_list=theta_S))\n",
        "  G_solver = (tf.train.RMSPropOptimizer(learning_rate=1e-4)\n",
        "              .minimize(G_loss, var_list=theta_G))\n",
        "  \n",
        "  clip_S = [p.assign(tf.clip_by_value(p, -0.01, 0.01)) for p in theta_S]\n",
        "  \n",
        "  ## Sessions\n",
        "  sess = tf.Session()\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "        \n",
        "  ## Iterations\n",
        "  while epsilon_hat < epsilon:      \n",
        "          \n",
        "    # 1. Train teacher models\n",
        "    teacher_models = list()\n",
        "    \n",
        "    for _ in range(k):\n",
        "                \n",
        "      Z_mb = sample_Z(partition_data_no, z_dim)\n",
        "      G_mb = sess.run(G_sample, feed_dict = {Z: Z_mb})\n",
        "                \n",
        "      temp_x = x_partition[i]\n",
        "      idx = np.random.permutation(len(temp_x[:, 0]))\n",
        "      X_mb = temp_x[idx[:partition_data_no], :]\n",
        "                \n",
        "      X_comb = np.concatenate((X_mb, G_mb), axis = 0)\n",
        "      Y_comb = np.concatenate((np.ones([partition_data_no,]), \n",
        "                               np.zeros([partition_data_no,])), axis = 0)\n",
        "                \n",
        "      model = LogisticRegression()\n",
        "      model.fit(X_comb, Y_comb)\n",
        "      teacher_models = teacher_models + [model]\n",
        "            \n",
        "    # 2. Student training\n",
        "    for _ in range(n_s):\n",
        "          \n",
        "      Z_mb = sample_Z(batch_size, z_dim)\n",
        "      G_mb = sess.run(G_sample, feed_dict = {Z: Z_mb})\n",
        "      Y_mb = list()\n",
        "            \n",
        "      for j in range(batch_size):                \n",
        "        n0, n1, r_j = pate_lamda(G_mb[j, :], teacher_models, lamda)\n",
        "        Y_mb = Y_mb + [r_j]\n",
        "       \n",
        "        # Update moments accountant\n",
        "        q = np.log(2 + lamda * abs(n0 - n1)) - np.log(4.0) - \\\n",
        "            (lamda * abs(n0 - n1))\n",
        "        q = np.exp(q)\n",
        "                \n",
        "        # Compute alpha\n",
        "        for l in range(L):\n",
        "          temp1 = 2 * (lamda**2) * (l+1) * (l+2)\n",
        "          temp2 = (1-q) * ( ((1-q)/(1-q*np.exp(2*lamda)))**(l+1) ) + \\\n",
        "                  q * np.exp(2*lamda * (l+1))\n",
        "          alpha[l] = alpha[l] + np.min([temp1, np.log(temp2)])\n",
        "        \n",
        "      # PATE labels for G_mb  \n",
        "      Y_mb = np.reshape(np.asarray(Y_mb), [-1,1])\n",
        "                \n",
        "      # Update student\n",
        "      _, D_loss_curr, _ = sess.run([S_solver, S_loss, clip_S], \n",
        "                                   feed_dict = {Z: Z_mb, Y: Y_mb})\n",
        "    \n",
        "    # Generator Update        \n",
        "    Z_mb = sample_Z(batch_size, z_dim)\n",
        "    _, G_loss_curr = sess.run([G_solver, G_loss], feed_dict = {Z: Z_mb})\n",
        "        \n",
        "    # epsilon_hat computation\n",
        "    curr_list = list()        \n",
        "    for l in range(L):\n",
        "      temp_alpha = (alpha[l] + np.log(1/delta)) / float(l+1)\n",
        "      curr_list = curr_list + [temp_alpha]\n",
        "        \n",
        "    epsilon_hat = np.min(curr_list)    \n",
        "\n",
        "  ## Outputs\n",
        "  x_train_hat = sess.run([G_sample], feed_dict = {Z: sample_Z(no, z_dim)})[0]\n",
        "    \n",
        "  return x_train_hat"
      ],
      "metadata": {
        "id": "uLBU2eWAjQw4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_eager_execution()\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sys\n",
        "sys.argv=['']\n",
        "del sys\n",
        "from data_generator import data_generator\n",
        "from utils import supervised_model_training\n",
        "from pate_gan import pategan\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "#%% \n",
        "def pategan_main (args):\n",
        "  \"\"\"PATEGAN Main function.\n",
        "  \n",
        "  Args:\n",
        "    data_no: number of generated data\n",
        "    data_dim: number of data dimensions\n",
        "    noise_rate: noise ratio on data\n",
        "    iterations: number of iterations for handling initialization randomness\n",
        "    n_s: the number of student training iterations\n",
        "    batch_size: the number of batch size for training student and generator\n",
        "    k: the number of teachers\n",
        "    epsilon, delta: Differential privacy parameters\n",
        "    lamda: noise size\n",
        "    \n",
        "  Returns:\n",
        "    - results: performances of Original and Synthetic performances\n",
        "    - train_data: original data\n",
        "    - synth_train_data: synthetically generated data\n",
        "  \"\"\"\n",
        "  \n",
        "  # Supervised model types\n",
        "  models = ['logisticregression','randomforest', 'gaussiannb','bernoullinb',\n",
        "            'svmlin', 'Extra Trees','LDA', 'AdaBoost','Bagging','gbm', 'xgb']\n",
        "  \n",
        "  # Data generation\n",
        "  if args.dataset == 'random':\n",
        "      train_data, test_data = data_generator(args.data_no, args.data_dim, \n",
        "                                         args.noise_rate)\n",
        "      data_dim = args.data_dim\n",
        "  elif args.dataset == 'credit':\n",
        "      # Insert relevant dataset here, and scale between 0 and 1.\n",
        "      data = pd.read_csv('creditcard.csv').to_numpy()\n",
        "      data = MinMaxScaler().fit_transform(data)\n",
        "      train_ratio = 0.5\n",
        "      train = np.random.rand(data.shape[0])<train_ratio \n",
        "      train_data, test_data = data[train], data[~train]\n",
        "      data_dim = data.shape[1]\n",
        "  \n",
        "  # Define outputs\n",
        "  results = np.zeros([len(models), 4])\n",
        "  \n",
        "  # Define PATEGAN parameters\n",
        "  parameters = {'n_s': args.n_s, 'batch_size': args.batch_size, 'k': args.k, \n",
        "                'epsilon': args.epsilon, 'delta': args.delta, \n",
        "                'lamda': args.lamda}\n",
        "  \n",
        "  # Generate synthetic training data\n",
        "  best_perf = 0.0\n",
        "  \n",
        "  for it in range(args.iterations):\n",
        "    print('Iteration',it)\n",
        "    synth_train_data_temp = pategan(train_data, parameters)\n",
        "    temp_perf, _ = supervised_model_training(\n",
        "        synth_train_data_temp[:, :(data_dim-1)], \n",
        "        np.round(synth_train_data_temp[:, (data_dim-1)]),\n",
        "        train_data[:, :(data_dim-1)], \n",
        "        np.round(train_data[:, (data_dim-1)]),\n",
        "        'logisticregression')\n",
        "    \n",
        "    # Select best synthetic data\n",
        "    if temp_perf > best_perf:\n",
        "      best_perf = temp_perf.copy()\n",
        "      synth_train_data = synth_train_data_temp.copy()\n",
        "      \n",
        "    print('Iteration: ' + str(it+1))\n",
        "    print('Best-Perf:' + str(best_perf))\n",
        "  \n",
        "  # Train supervised models\n",
        "  for model_index in range(len(models)):\n",
        "    model_name = models[model_index]\n",
        "    \n",
        "    # Using original data\n",
        "    results[model_index, 0], results[model_index, 2] = (\n",
        "        supervised_model_training(train_data[:, :(data_dim-1)], \n",
        "                                  np.round(train_data[:, (data_dim-1)]),\n",
        "                                  test_data[:, :(data_dim-1)], \n",
        "                                  np.round(test_data[:, (data_dim-1)]),\n",
        "                                  model_name))\n",
        "        \n",
        "    # Using synthetic data\n",
        "    results[model_index, 1], results[model_index, 3] = (\n",
        "        supervised_model_training(synth_train_data[:, :(data_dim-1)], \n",
        "                                  np.round(synth_train_data[:, (data_dim-1)]),\n",
        "                                  test_data[:, :(data_dim-1)], \n",
        "                                  np.round(test_data[:, (data_dim-1)]),\n",
        "                                  model_name))\n",
        "\n",
        "    \n",
        "    \n",
        "  # Print the results for each iteration\n",
        "  results = pd.DataFrame(np.round(results, 4), \n",
        "                         columns=['AUC-Original', 'AUC-Synthetic', \n",
        "                                  'APR-Original', 'APR-Synthetic'])\n",
        "  print(results)\n",
        "  print('Averages:')\n",
        "  print(results.mean(axis=0))\n",
        "  \n",
        "  return results, train_data, synth_train_data\n",
        "\n",
        "  \n",
        "#%%  \n",
        "if __name__ == '__main__':\n",
        "  \n",
        "  # Inputs for the main function\n",
        "  parser = argparse.ArgumentParser()\n",
        "  parser.add_argument(\n",
        "      '--data_no',\n",
        "      help='number of generated data',\n",
        "      default=10000,\n",
        "      type=int)\n",
        "  parser.add_argument(\n",
        "      '--data_dim',\n",
        "      help='number of dimensions of generated dimension (if random)',\n",
        "      default=10,\n",
        "      type=int)\n",
        "  parser.add_argument(\n",
        "      '--dataset',\n",
        "      help='dataset to use',\n",
        "      default='random',\n",
        "      type=str)\n",
        "  parser.add_argument(\n",
        "      '--noise_rate',\n",
        "      help='noise ratio on data',\n",
        "      default=1.0,\n",
        "      type=float)\n",
        "  parser.add_argument(\n",
        "      '--iterations',\n",
        "      help='number of iterations for handling initialization randomness',\n",
        "      default=50,\n",
        "      type=int)\n",
        "  parser.add_argument(\n",
        "      '--n_s',\n",
        "      help='the number of student training iterations',\n",
        "      default=1,\n",
        "      type=int)\n",
        "  parser.add_argument(\n",
        "      '--batch_size',\n",
        "      help='the number of batch size for training student and generator',\n",
        "      default=64,\n",
        "      type=int)\n",
        "  parser.add_argument(\n",
        "      '--k',\n",
        "      help='the number of teachers',\n",
        "      default=10,\n",
        "      type=float)\n",
        "  parser.add_argument(\n",
        "      '--epsilon',\n",
        "      help='Differential privacy parameters (epsilon)',\n",
        "      default=1.0,\n",
        "      type=float)\n",
        "  parser.add_argument(\n",
        "      '--delta',\n",
        "      help='Differential privacy parameters (delta)',\n",
        "      default=0.00001,\n",
        "      type=float)\n",
        "  parser.add_argument(\n",
        "      '--lamda',\n",
        "      help='PATE noise size',\n",
        "      default=1.0,\n",
        "      type=float)\n",
        "  \n",
        "  \n",
        "  args = parser.parse_args() \n",
        "  parser.add_argument('-f')\n",
        "\n",
        "  \n",
        "  # Calls main function  \n",
        "  results, ori_data, synth_data = pategan_main(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzEuQlYmjXl5",
        "outputId": "23defa64-833a-4c7f-a00c-0ccd6bd9f043"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0\n",
            "WARNING:tensorflow:From /content/mlforhealthlabpub/alg/pategan/pate_gan.py:68: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /content/mlforhealthlabpub/alg/pategan/pate_gan.py:68: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /content/mlforhealthlabpub/alg/pategan/pate_gan.py:124: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /content/mlforhealthlabpub/alg/pategan/pate_gan.py:124: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /content/mlforhealthlabpub/alg/pategan/pate_gan.py:116: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /content/mlforhealthlabpub/alg/pategan/pate_gan.py:116: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /content/mlforhealthlabpub/alg/pategan/pate_gan.py:173: The name tf.train.RMSPropOptimizer is deprecated. Please use tf.compat.v1.train.RMSPropOptimizer instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /content/mlforhealthlabpub/alg/pategan/pate_gan.py:173: The name tf.train.RMSPropOptimizer is deprecated. Please use tf.compat.v1.train.RMSPropOptimizer instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /content/mlforhealthlabpub/alg/pategan/pate_gan.py:181: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /content/mlforhealthlabpub/alg/pategan/pate_gan.py:181: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /content/mlforhealthlabpub/alg/pategan/pate_gan.py:182: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /content/mlforhealthlabpub/alg/pategan/pate_gan.py:182: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 1\n",
            "Best-Perf:0.5598131449177359\n",
            "Iteration 1\n",
            "Iteration: 2\n",
            "Best-Perf:0.7436221842059335\n",
            "Iteration 2\n",
            "Iteration: 3\n",
            "Best-Perf:0.7436221842059335\n",
            "Iteration 3\n",
            "Iteration: 4\n",
            "Best-Perf:0.7436221842059335\n",
            "Iteration 4\n",
            "Iteration: 5\n",
            "Best-Perf:0.7995233055568115\n",
            "Iteration 5\n",
            "Iteration: 6\n",
            "Best-Perf:0.7995233055568115\n",
            "Iteration 6\n",
            "Iteration: 7\n",
            "Best-Perf:0.7995233055568115\n",
            "Iteration 7\n",
            "Iteration: 8\n",
            "Best-Perf:0.7995233055568115\n",
            "Iteration 8\n",
            "Iteration: 9\n",
            "Best-Perf:0.7995233055568115\n",
            "Iteration 9\n",
            "Iteration: 10\n",
            "Best-Perf:0.7995233055568115\n",
            "Iteration 10\n",
            "Iteration: 11\n",
            "Best-Perf:0.7995233055568115\n",
            "Iteration 11\n",
            "Iteration: 12\n",
            "Best-Perf:0.7995233055568115\n",
            "Iteration 12\n",
            "Iteration: 13\n",
            "Best-Perf:0.7995233055568115\n",
            "Iteration 13\n",
            "Iteration: 14\n",
            "Best-Perf:0.7995233055568115\n",
            "Iteration 14\n",
            "Iteration: 15\n",
            "Best-Perf:0.7995233055568115\n",
            "Iteration 15\n",
            "Iteration: 16\n",
            "Best-Perf:0.7995233055568115\n",
            "Iteration 16\n",
            "Iteration: 17\n",
            "Best-Perf:0.7995233055568115\n",
            "Iteration 17\n",
            "Iteration: 18\n",
            "Best-Perf:0.7995233055568115\n",
            "Iteration 18\n",
            "Iteration: 19\n",
            "Best-Perf:0.7995233055568115\n",
            "Iteration 19\n",
            "Iteration: 20\n",
            "Best-Perf:0.7995233055568115\n",
            "Iteration 20\n",
            "Iteration: 21\n",
            "Best-Perf:0.7995233055568115\n",
            "Iteration 21\n",
            "Iteration: 22\n",
            "Best-Perf:0.7995233055568115\n",
            "Iteration 22\n",
            "Iteration: 23\n",
            "Best-Perf:0.7995233055568115\n",
            "Iteration 23\n",
            "Iteration: 24\n",
            "Best-Perf:0.7995233055568115\n",
            "Iteration 24\n",
            "Iteration: 25\n",
            "Best-Perf:0.7995233055568115\n",
            "Iteration 25\n",
            "Iteration: 26\n",
            "Best-Perf:0.7995233055568115\n",
            "Iteration 26\n",
            "Iteration: 27\n",
            "Best-Perf:0.7995233055568115\n",
            "Iteration 27\n",
            "Iteration: 28\n",
            "Best-Perf:0.7995233055568115\n",
            "Iteration 28\n",
            "Iteration: 29\n",
            "Best-Perf:0.7995233055568115\n",
            "Iteration 29\n",
            "Iteration: 30\n",
            "Best-Perf:0.7995233055568115\n",
            "Iteration 30\n",
            "Iteration: 31\n",
            "Best-Perf:0.7995233055568115\n",
            "Iteration 31\n",
            "Iteration: 32\n",
            "Best-Perf:0.7995233055568115\n",
            "Iteration 32\n",
            "Iteration: 33\n",
            "Best-Perf:0.7995233055568115\n",
            "Iteration 33\n",
            "Iteration: 34\n",
            "Best-Perf:0.7995233055568115\n",
            "Iteration 34\n",
            "Iteration: 35\n",
            "Best-Perf:0.7995233055568115\n",
            "Iteration 35\n",
            "Iteration: 36\n",
            "Best-Perf:0.7995233055568115\n",
            "Iteration 36\n",
            "Iteration: 37\n",
            "Best-Perf:0.7995233055568115\n",
            "Iteration 37\n",
            "Iteration: 38\n",
            "Best-Perf:0.7995233055568115\n",
            "Iteration 38\n",
            "Iteration: 39\n",
            "Best-Perf:0.7995233055568115\n",
            "Iteration 39\n",
            "Iteration: 40\n",
            "Best-Perf:0.7995233055568115\n",
            "Iteration 40\n",
            "Iteration: 41\n",
            "Best-Perf:0.7995233055568115\n",
            "Iteration 41\n",
            "Iteration: 42\n",
            "Best-Perf:0.7995233055568115\n",
            "Iteration 42\n",
            "Iteration: 43\n",
            "Best-Perf:0.8236928578801721\n",
            "Iteration 43\n",
            "Iteration: 44\n",
            "Best-Perf:0.8236928578801721\n",
            "Iteration 44\n",
            "Iteration: 45\n",
            "Best-Perf:0.8236928578801721\n",
            "Iteration 45\n",
            "Iteration: 46\n",
            "Best-Perf:0.8236928578801721\n",
            "Iteration 46\n",
            "Iteration: 47\n",
            "Best-Perf:0.8236928578801721\n",
            "Iteration 47\n",
            "Iteration: 48\n",
            "Best-Perf:0.8236928578801721\n",
            "Iteration 48\n",
            "Iteration: 49\n",
            "Best-Perf:0.8236928578801721\n",
            "Iteration 49\n",
            "Iteration: 50\n",
            "Best-Perf:0.8236928578801721\n",
            "[01:59:47] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[01:59:49] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "    AUC-Original  AUC-Synthetic  APR-Original  APR-Synthetic\n",
            "0         0.9066         0.8319        0.8978         0.8103\n",
            "1         0.8859         0.7615        0.8738         0.7124\n",
            "2         0.8422         0.8005        0.8229         0.7750\n",
            "3         0.5007         0.4993        0.4654         0.4650\n",
            "4         0.9067         0.8327        0.8977         0.8112\n",
            "5         0.8759         0.7778        0.8536         0.7275\n",
            "6         0.9067         0.8314        0.8977         0.8095\n",
            "7         0.8923         0.8044        0.8774         0.7800\n",
            "8         0.8527         0.7429        0.8210         0.6841\n",
            "9         0.8969         0.7987        0.8860         0.7652\n",
            "10        0.8946         0.7958        0.8828         0.7645\n",
            "Averages:\n",
            "AUC-Original     0.851018\n",
            "AUC-Synthetic    0.770627\n",
            "APR-Original     0.834191\n",
            "APR-Synthetic    0.736791\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%tb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "6Hw55o_mkg_0",
        "outputId": "785bf1e4-82be-489e-c844-dd0baa8da482"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-b632dc825276>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m   \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m   \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-f'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/argparse.py\u001b[0m in \u001b[0;36mparse_args\u001b[0;34m(self, args, namespace)\u001b[0m\n\u001b[1;32m   1765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1766\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unrecognized arguments: %s'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1767\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1768\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1769\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/argparse.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m   2515\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2516\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'prog'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'message'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2517\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%(prog)s: error: %(message)s\\n'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.7/argparse.py\u001b[0m in \u001b[0;36mexit\u001b[0;34m(self, status, message)\u001b[0m\n\u001b[1;32m   2502\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2503\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_print_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2504\u001b[0;31m         \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2506\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSystemExit\u001b[0m: 2"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%tb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "Gs_nTgTXjiJ8",
        "outputId": "b83516f9-3251-4a20-c40f-99fe12da07a8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-307b2a929bce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    174\u001b[0m       type=float)\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m   \u001b[0;31m# Calls main function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/argparse.py\u001b[0m in \u001b[0;36mparse_args\u001b[0;34m(self, args, namespace)\u001b[0m\n\u001b[1;32m   1765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1766\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unrecognized arguments: %s'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1767\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1768\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1769\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/argparse.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m   2515\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2516\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'prog'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'message'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2517\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%(prog)s: error: %(message)s\\n'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.7/argparse.py\u001b[0m in \u001b[0;36mexit\u001b[0;34m(self, status, message)\u001b[0m\n\u001b[1;32m   2502\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2503\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_print_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2504\u001b[0;31m         \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2506\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSystemExit\u001b[0m: 2"
          ]
        }
      ]
    }
  ]
}